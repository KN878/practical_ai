{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from collections import defaultdict\n",
    "from nltk import tokenize\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import numpy as np\n",
    "import copy\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from numpy.linalg import svd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try simple engish\n",
    "textfile = \"facts.txt\"\n",
    "\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "sentences = []\n",
    "init_sentences = []\n",
    "words = []\n",
    "lexemes = []\n",
    "with open(textfile, encoding='utf-8', mode='r') as f:\n",
    "    text = f.read().lower()\n",
    "    \n",
    "    # lets split text for sentences first\n",
    "    \n",
    "    # these 2 parts are the same. Either complex one:\n",
    "    tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "    sentences = tokenize.sent_tokenize(text)\n",
    "    init_sentences = copy.deepcopy(sentences)\n",
    "    # let's explode sentences to lexemes\n",
    "    for i, sentence in enumerate(sentences):\n",
    "        if not sentence:\n",
    "            continue\n",
    "        s_words = [word for word\n",
    "                    in tokenize.word_tokenize(sentence)\n",
    "                    if word not in (',', '.', ':', '-', ';', '?', '!', '\"', \"``\", \"`\", \"''\")\n",
    "                ]\n",
    "        s_lexemes = [stemmer.stem(word) for word in s_words]\n",
    "        for j, word in enumerate(s_words):\n",
    "            sentences[i] = sentences[i].replace(word, s_lexemes[j])\n",
    "        words.append(s_words)\n",
    "        lexemes.append(s_lexemes)\n",
    "#flattening lexemes list\n",
    "lexemes = [item for sublist in lexemes for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf-idf vectorization\n",
    "vectorizer = TfidfVectorizer()\n",
    "tdm = vectorizer.fit_transform(sentences).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PCA\n",
    "pca = PCA(n_components = 0.8,  svd_solver=\"full\")\n",
    "tdm_reduced = pca.fit_transform(tdm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_query(query, facts_tdm):\n",
    "        query = query.lower()\n",
    "        query_words = [word for word\n",
    "                    in tokenize.word_tokenize(query)\n",
    "                    if word not in (',', '.', ':', '-', ';', '?', '!', '\"', \"``\", \"`\", \"''\")\n",
    "                ]\n",
    "        query_lexemes = [stemmer.stem(word) for word in query_words]\n",
    "        for j, word in enumerate(query_words):\n",
    "            query = query.replace(word, query_lexemes[j])\n",
    "        queries_list = []\n",
    "        queries_list.append(query)\n",
    "        query_tdm = vectorizer.transform(queries_list).toarray()\n",
    "        query_tdm_reduced = pca.transform(query_tdm)\n",
    "        similarity = cosine_similarity(facts_tdm, query_tdm_reduced).reshape(-1)\n",
    "        top_matches_idx = np.argsort(similarity)[::-1][:5]\n",
    "        return top_matches_idx    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dcg(found_idx, relevant_idx):\n",
    "    def is_relevant(idx):\n",
    "        return int(idx in relevant_idx)\n",
    "    \n",
    "    dcg = 0\n",
    "    for i, idx in enumerate(found_idx, 1):\n",
    "        dcg += is_relevant(idx)/np.log2(i+1)\n",
    "    return dcg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pFound(found_idx, relevant_idx, max_rel, p_break):\n",
    "    def compute_relevance():\n",
    "        rel = []\n",
    "        for idx in found_idx:\n",
    "            rel.append(max_rel if idx in relevant_idx else 0)\n",
    "        return rel\n",
    "    \n",
    "    def pLook(i):\n",
    "        return p_look[i-1]*(1 - p_rel[i-1])*(1 - p_break)\n",
    "    \n",
    "    p_look = [1]\n",
    "    p_rel = compute_relevance()\n",
    "    pFound = 0\n",
    "    \n",
    "    for i, idx in enumerate(found_idx):\n",
    "        if i != 0:\n",
    "            p_look.append(pLook(i))\n",
    "        pFound += p_look[i]*p_rel[i]\n",
    "    return pFound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = []\n",
    "relevance = []\n",
    "with open('queries.txt', mode='r') as f:\n",
    "    queries = [line.strip() for line in f.readlines()]\n",
    "with open('relevance.txt', mode='r') as f:\n",
    "    for line in f.readlines():\n",
    "        line = line.strip()\n",
    "        relevance.append(list(map(int, line.split(','))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[157  38  15 143   2]\n",
      "[2, 15, 38, 143, 144, 157]\n",
      "Query: person, DCG: 2.9484591188793923, pFound: 0.7881612040000001\n",
      "[133  90   9 139   1]\n",
      "[1, 5, 9, 90, 133, 139]\n",
      "Query: planet, DCG: 2.9484591188793923, pFound: 0.7881612040000001\n",
      "[147  12  15   2  80]\n",
      "[2, 12, 15, 80, 147]\n",
      "Query: average, DCG: 2.9484591188793923, pFound: 0.7881612040000001\n",
      "[ 26 155  22 143   9]\n",
      "[9, 10, 22, 26, 31, 99, 118, 143, 155, 159]\n",
      "Query: one, DCG: 2.9484591188793923, pFound: 0.7881612040000001\n",
      "[ 22  46 139  19   3]\n",
      "[3, 19, 22, 35, 46, 69, 79, 80, 103, 114, 118, 139]\n",
      "Query: year, DCG: 2.9484591188793923, pFound: 0.7881612040000001\n",
      "[ 74 165 149 126  83]\n",
      "[9, 12, 74, 83, 126, 140, 149, 165, 166]\n",
      "Query: human, DCG: 2.9484591188793923, pFound: 0.7881612040000001\n",
      "[ 29 165  53   9 103]\n",
      "[9, 29, 53, 103, 130, 142, 164, 165]\n",
      "Query: every, DCG: 2.9484591188793923, pFound: 0.7881612040000001\n",
      "[152 104 130   2  35]\n",
      "[2, 35, 104, 130, 152]\n",
      "Query: call, DCG: 2.9484591188793923, pFound: 0.7881612040000001\n",
      "[ 11  78 165  89 137]\n",
      "[10, 11, 78, 89, 137, 165]\n",
      "Query: million, DCG: 2.9484591188793923, pFound: 0.7881612040000001\n",
      "[111  38  46  58 107]\n",
      "[5, 10, 38, 46, 58, 80, 89, 100, 103, 104, 107, 111]\n",
      "Query: people, DCG: 2.9484591188793923, pFound: 0.7881612040000001\n"
     ]
    }
   ],
   "source": [
    "for i, query in enumerate(queries):\n",
    "    matches_idx = search_query(query, tdm_reduced)\n",
    "    print(matches_idx)\n",
    "    print(relevance[i])\n",
    "    _dcg = dcg(matches_idx, relevance[i])\n",
    "    _pFound = pFound(matches_idx, relevance[i], 0.4, 0.15)\n",
    "    print(\"Query: {}, DCG: {}, pFound: {}\".format(query, _dcg, _pFound))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
