{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['space',\n",
       " 'languages',\n",
       " 'geography',\n",
       " 'easter',\n",
       " 'cryptocurrency',\n",
       " 'chemistry',\n",
       " 'Internet',\n",
       " 'social networks',\n",
       " 'ancient Rome',\n",
       " 'ships',\n",
       " 'electricity',\n",
       " 'ancient world',\n",
       " 'geology',\n",
       " 'religion',\n",
       " 'travel',\n",
       " 'memes',\n",
       " 'education',\n",
       " 'vaccinations',\n",
       " 'computers',\n",
       " 'flu',\n",
       " 'google',\n",
       " 'browsers',\n",
       " 'games',\n",
       " 'easter eggs',\n",
       " 'search engines',\n",
       " 'English',\n",
       " 'ancient Greece',\n",
       " 'health',\n",
       " 'smartphones',\n",
       " 'psychology',\n",
       " 'psychotherapy']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topics = \"\"\"space\n",
    "languages\n",
    "geography\n",
    "easter\n",
    "cryptocurrency\n",
    "chemistry\n",
    "Internet\n",
    "social networks\n",
    "ancient Rome\n",
    "ships\n",
    "electricity\n",
    "ancient world\n",
    "geology\n",
    "religion\n",
    "travel\n",
    "memes\n",
    "education\n",
    "vaccinations\n",
    "computers\n",
    "flu\n",
    "google\n",
    "browsers\n",
    "games\n",
    "easter eggs\n",
    "search engines\n",
    "English\n",
    "ancient Greece\n",
    "health\n",
    "smartphones\n",
    "psychology\n",
    "psychotherapy\"\"\".split('\\n')\n",
    "\n",
    "topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec = Word2Vec.load('model/word2vec.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_embeddings = []\n",
    "\n",
    "for topic in topics:\n",
    "    word_embeddings = []\n",
    "    for word in topic.split(' '):\n",
    "        # apparently, the word2vec model is caseless\n",
    "        word_embeddings.append(word2vec.wv[word.lower()])\n",
    "            \n",
    "    topic_embedding = np.mean(word_embeddings, axis=0)\n",
    "    topic_embeddings.append(topic_embedding)\n",
    "\n",
    "topic_embeddings = np.array(topic_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(array([0., 0.]), array([1., 1.])),\n",
       " (array([1., 1., 1.]), array([0., 0., 0.]))]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([[(np.zeros(2), np.ones(2))], [(np.ones(3), np.zeros(3))]], [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dunn_index_max(x, labels, cluster_centers):\n",
    "    max_distance_within_cluster = 1e-6\n",
    "    for label in np.unique(labels):\n",
    "        x_label = x[labels == label]\n",
    "        n_elements = len(x_label)\n",
    "        if n_elements < 2:\n",
    "            continue\n",
    "        x_times_x = [[(x_label[i], x_label[j]) for j in range(i + 1, n_elements)] for i in range(n_elements)]\n",
    "        # flatten\n",
    "        x_times_x = sum(x_times_x, [])\n",
    "        distances = [np.linalg.norm(pair[0] - pair[1]) for pair in x_times_x]\n",
    "        max_distance_label = np.amax(distances)\n",
    "        if max_distance_label > max_distance_within_cluster:\n",
    "            max_distance_within_cluster = max_distance_label\n",
    "    \n",
    "    n_clusters = len(cluster_centers)\n",
    "    c_times_c = [[(cluster_centers[i], cluster_centers[j]) for j in range(i + 1, n_clusters)] for i in range(n_clusters)]\n",
    "    # flatten\n",
    "    c_times_c = sum(c_times_c, [])\n",
    "    cluster_distances = [np.linalg.norm(pair[0] - pair[1]) for pair in c_times_c]\n",
    "    min_distance_centers = np.amin(cluster_distances)\n",
    "    \n",
    "    return min_distance_centers / max_distance_within_cluster\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 games\n",
      "2 memes\n",
      "3 ancient world\n",
      "4 cryptocurrency\n",
      "5 vaccinations\n",
      "6 Internet\n"
     ]
    }
   ],
   "source": [
    "best_centers = None\n",
    "best_score = 0\n",
    "for n_clusters in range(5, 11):\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42).fit(topic_embeddings)\n",
    "    dunn_index = dunn_index_max(topic_embeddings, kmeans.labels_, kmeans.cluster_centers_)\n",
    "    if dunn_index > best_score:\n",
    "        best_score = dunn_index\n",
    "        best_centers = kmeans.cluster_centers_\n",
    "        \n",
    "for i, center in enumerate(best_centers):\n",
    "    nearest_neighbour_idx = np.argmin(np.linalg.norm(topic_embeddings - center, axis=1))\n",
    "    print(i + 1, topics[nearest_neighbour_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
