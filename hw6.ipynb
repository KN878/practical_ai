{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec\n",
    "import nltk\n",
    "from collections import defaultdict\n",
    "from nltk import tokenize\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import numpy as np\n",
    "import copy\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from numpy.linalg import svd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Doc2Vec.load('apnews_dbow/doc2vec.bin', mmap=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try simple engish\n",
    "textfile = \"facts.txt\"\n",
    "\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "sentences = []\n",
    "init_sentences = []\n",
    "words = []\n",
    "lexemes = []\n",
    "with open(textfile, encoding='utf-8', mode='r') as f:\n",
    "    text = f.read().lower()\n",
    "    \n",
    "    # lets split text for sentences first\n",
    "    \n",
    "    # these 2 parts are the same. Either complex one:\n",
    "    tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "    sentences = tokenize.sent_tokenize(text)\n",
    "    init_sentences = copy.deepcopy(sentences)\n",
    "    # let's explode sentences to lexemes\n",
    "    for i, sentence in enumerate(sentences):\n",
    "        if not sentence:\n",
    "            continue\n",
    "        s_words = [word for word\n",
    "                    in tokenize.word_tokenize(sentence)\n",
    "                    if word not in (',', '.', ':', '-', ';', '?', '!', '\"', \"``\", \"`\", \"''\")\n",
    "                ]\n",
    "        s_lexemes = [stemmer.stem(word) for word in s_words]\n",
    "        for j, word in enumerate(s_words):\n",
    "            sentences[i] = sentences[i].replace(word, s_lexemes[j])\n",
    "        words.append(s_words)\n",
    "        lexemes.append(s_lexemes)\n",
    "#flattening lexemes list\n",
    "lexemes = [item for sublist in lexemes for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf-idf vectorization\n",
    "vectorizer = TfidfVectorizer()\n",
    "tdm = vectorizer.fit_transform(sentences).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PCA\n",
    "pca = PCA(n_components = 0.8,  svd_solver=\"full\")\n",
    "tdm_reduced = pca.fit_transform(tdm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_query(query, facts_tdm):\n",
    "        query = query.lower()\n",
    "        query_words = [word for word\n",
    "                    in tokenize.word_tokenize(query)\n",
    "                    if word not in (',', '.', ':', '-', ';', '?', '!', '\"', \"``\", \"`\", \"''\")\n",
    "                ]\n",
    "        query_lexemes = [stemmer.stem(word) for word in query_words]\n",
    "        for j, word in enumerate(query_words):\n",
    "            query = query.replace(word, query_lexemes[j])\n",
    "        queries_list = []\n",
    "        queries_list.append(query)\n",
    "        query_tdm = vectorizer.transform(queries_list).toarray()\n",
    "        query_tdm_reduced = pca.transform(query_tdm)\n",
    "        similarity = cosine_similarity(facts_tdm, query_tdm_reduced).reshape(-1)\n",
    "        top_matches_idx = np.argsort(similarity)[::-1][:5]\n",
    "        return [init_sentences[i] for i in top_matches_idx]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import locale\n",
    "locale.setlocale(locale.LC_ALL, '')\n",
    "\n",
    "# Api key found on https://translate.yandex.com/developers/keys\n",
    "api_key = open(\"yandex.translate.key\").read()   # todo your key in the file\n",
    "\n",
    "from yandex.Translater import Translater\n",
    "tr = Translater()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found:\n",
      "146. во Франции, это законно выйти замуж за мертвого человека.\n",
      "8. в 1386 г. свинья во Франции был казнен путем публичного повешения за убийство ребенка.\n",
      "140. то, что называется “французским поцелуем” в англоязычных странах известен как “английский поцелуй” во Франции.\n",
      "1.\n",
      "86. большинство частиц пыли в вашем доме сделаны из мертвой кожи!\n"
     ]
    }
   ],
   "source": [
    "query = \"франция\"\n",
    "tr.set_key(api_key)\n",
    "tr.set_from_lang('ru')\n",
    "tr.set_to_lang('en')\n",
    "\n",
    "tr.set_text(query)\n",
    "result = tr.translate()\n",
    "\n",
    "matches = search_query(result, tdm_reduced)\n",
    "print(\"Found:\")\n",
    "tr.set_from_lang('en')\n",
    "tr.set_to_lang('ru')\n",
    "for match in matches:\n",
    "    tr.set_text(match)\n",
    "    print(tr.translate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
