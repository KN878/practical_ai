{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "plt.rcParams[\"figure.figsize\"] = (10, 7) # (w, h)\n",
    "import numpy as np\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "vidcap = cv2.VideoCapture('cat.mp4')\n",
    "\n",
    "def getFrame(sec, state):\n",
    "    vidcap.set(cv2.CAP_PROP_POS_MSEC,sec*1000)\n",
    "    hasFrames,image = vidcap.read()\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    if hasFrames:\n",
    "        state.append(image)\n",
    "    return hasFrames\n",
    "\n",
    "sec = 3.5\n",
    "frameRate = 0.3\n",
    "state = []\n",
    "success = getFrame(sec, state)\n",
    "# used frame till 20 second, becuase after 20th second occurs background clutter -> the model is not able to detect the cat\n",
    "while sec < 20:\n",
    "    sec = sec + frameRate\n",
    "    sec = round(sec, 2)\n",
    "    success = getFrame(sec,state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I use model from https://github.com/alisonswu/cat-detector just a little bit changed detect function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finding object:   0%|          | 0/56 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'scipy.misc' has no attribute 'imresize'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-4ffdf2729288>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0mbboxes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Finding object\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0mbboxes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdetect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatdetector\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/practicalAI/exam1/pai_exam/17_Tracking/utils.py\u001b[0m in \u001b[0;36mdetect\u001b[0;34m(image_name, net, c_threshold, overlap_threshold)\u001b[0m\n\u001b[1;32m    182\u001b[0m     \u001b[0mhzoom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mori_H\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m448\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m     \u001b[0mwzoom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mori_W\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m448\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 184\u001b[0;31m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfromarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mori_img\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m448\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m448\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    185\u001b[0m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m255.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0mimg\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0.485\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.456\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.406\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'scipy.misc' has no attribute 'imresize'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "from model import catdetector\n",
    "from utils import detect\n",
    "from tqdm import tqdm\n",
    "\n",
    "if not os.path.exists('catdetector.pkl'):\n",
    "    os.system(\"wget https://s3.amazonaws.com/cat-detector/catdetector.pkl\")\n",
    "    \n",
    "\n",
    "save_model = torch.load('catdetector.pkl', map_location='cpu')\n",
    "model_dict =  catdetector.state_dict()\n",
    "state_dict = {k:v for k,v in save_model.items() if k in model_dict.keys()}\n",
    "model_dict.update(state_dict)\n",
    "catdetector.load_state_dict(model_dict)\n",
    "\n",
    "_ = catdetector.eval()\n",
    "\n",
    "bboxes = []\n",
    "for i in tqdm(range(len(state)), desc=\"Finding object\"):\n",
    "    bboxes.append(detect(state[i], catdetector))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Drawing graphics: 100%|██████████████████████████████████████████████████████████████| 56/56 [00:00<00:00, 1749.97it/s]\n"
     ]
    }
   ],
   "source": [
    "def find_bb_center(bbox):\n",
    "    if bbox is None:\n",
    "        return None\n",
    "\n",
    "    half_w = int(abs(bbox[2] - bbox[0]) / 2)\n",
    "    half_h = int(abs(bbox[3] - bbox[1]) / 2)\n",
    "\n",
    "    return (bbox[0] + half_w, bbox[1] + half_h)\n",
    "\n",
    "def most_frequent(List): \n",
    "    return max(set(List), key = List.count) \n",
    "\n",
    "def off_px(p, offset):\n",
    "    ''' offset point by X axis '''\n",
    "    return (p[0] + offset, p[1])\n",
    "\n",
    "centers = []\n",
    "for bbox in bboxes:\n",
    "    centers.append(find_bb_center(bbox))\n",
    "\n",
    "centers_to_check = 6 # should be even\n",
    "direction_right = 1\n",
    "direction_left = -1\n",
    "\n",
    "for_video = []\n",
    "for i in tqdm(range(len(state)), desc=\"Drawing graphics\"):\n",
    "    cv2img = state[i].copy()\n",
    "    ori_H, ori_W, _ = cv2img.shape\n",
    "    \n",
    "    bbox = bboxes[i]\n",
    "    if bbox is None:\n",
    "        for_video.append(cv2img)\n",
    "        continue\n",
    "    \n",
    "    # draw bounding box\n",
    "    cv2.rectangle(cv2img,(bbox[0], bbox[1]), (bbox[2], bbox[3]), (255,0,0), thickness=int(ori_H/150))\n",
    "\n",
    "    last_centers = [i for i in centers[:i+1] if i] # remove any None centers\n",
    "    if len(last_centers) >= centers_to_check:\n",
    "        current_p = last_centers[-1] # find current center\n",
    "\n",
    "        last_centers = last_centers[-centers_to_check:] # leave only last `centers_to_check` centers\n",
    "        last_centers.reverse() # make them in order current -> old -> older\n",
    "\n",
    "        directions = [] # list of last `centers_to_check`-1 direction changes\n",
    "        for ci in range(len(last_centers) - 1):\n",
    "            cc = last_centers[ci]\n",
    "            pc = last_centers[ci + 1]\n",
    "\n",
    "            # detecting direction change\n",
    "            if cc[0] < pc[1]:\n",
    "                directions.append(direction_right) # moving right\n",
    "            else:\n",
    "                directions.append(direction_left) # moving left\n",
    "        direction = most_frequent(directions) # find most common direction change in the last row\n",
    "\n",
    "        # we want fancy line\n",
    "        bb_width = abs(bbox[2] - bbox[0])\n",
    "        l_offset = int(bb_width * 0.2)\n",
    "        arr_width = int(bb_width * 0.5) + l_offset\n",
    "        \n",
    "        # draw arrow from current center with detected direction\n",
    "        cv2.arrowedLine(cv2img, off_px(current_p, direction * l_offset), off_px(current_p, direction * arr_width), (0, 255, 0), 2)\n",
    "        #cv2.circle(cv2img, current_p, 3, (0, 255, 0), -1)\n",
    "\n",
    "    for_video.append(cv2img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "height, width, layers = for_video[0].shape\n",
    "size = (width,height)    \n",
    "\n",
    "out = cv2.VideoWriter('cat_tracking.avi',cv2.VideoWriter_fourcc(*'DIVX'), 15, size)\n",
    "\n",
    "for img in for_video:\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
    "    out.write(img)\n",
    "out.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
